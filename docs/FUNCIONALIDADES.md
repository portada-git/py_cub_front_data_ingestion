# Funcionalidades del Sistema PortAda

## üìã Resumen Ejecutivo

Sistema completo de ingesta y an√°lisis de datos hist√≥ricos de peri√≥dicos usando la librer√≠a PortAda con Delta Lake.

---

## üîÑ BACKEND (FastAPI + PortAda)

### 1. Ingesta de Datos

#### 1.1 Carga de Datos de Extracci√≥n
- **Endpoint**: `POST /api/ingestion/upload`
- **Tipo**: `extraction`
- **Formato**: JSON
- **Proceso**:
  - Sube archivo JSON con datos extra√≠dos de peri√≥dicos
  - Almacena temporalmente en `<ingestion_folder>/<periodico>/<user_name>/<fichero_temporal>`
  - Procesa as√≠ncronamente con background tasks
  - Usa `layer_news.ingest(data_path_delta_lake, local_path=file_path)`
  - ‚ö†Ô∏è **IMPORTANTE**: El archivo fuente se elimina despu√©s de la ingesta

#### 1.2 Carga de Entidades Conocidas
- **Endpoint**: `POST /api/ingestion/upload`
- **Tipo**: `known_entities`
- **Formato**: YAML
- **Proceso**:
  - Sube archivo YAML con entidades conocidas
  - Usa `layer_entities.copy_ingested_entities()` y `save_raw_entities()`
  - Procesa as√≠ncronamente

#### 1.3 Monitoreo de Ingesta
- **Endpoint**: `GET /api/ingestion/status/{task_id}`
- **Retorna**:
  - Estado: pending, processing, completed, failed
  - Progreso (0-100%)
  - Registros procesados
  - Mensajes de error si aplica

---

### 2. An√°lisis de Datos

#### 2.1 Fechas Faltantes
- **Endpoint**: `POST /api/analysis/missing-dates`
- **Par√°metros**:
  - `publication_name`: Peri√≥dico (DB, DM, SM, etc.) - **OBLIGATORIO**
  - `query_mode`: "file" o "date_range"
  - **Modo File**:
    - `date_and_edition_list`: Contenido del archivo (YAML, JSON o lista)
  - **Modo Date Range**:
    - `start_date`: Fecha inicio (YYYY-MM-DD) - opcional
    - `end_date`: Fecha final (YYYY-MM-DD) - opcional
- **Librer√≠a PortAda**:
  ```python
  layer_news.get_missing_dates_from_a_newspaper(
      data_path,
      publication_name=publication_name
  )
  ```
- **Retorna**: Lista de fechas y ediciones faltantes con duraci√≥n del gap

#### 2.2 Duplicados
- **Endpoint Master**: `POST /api/analysis/duplicates`
- **Par√°metros** (todos opcionales):
  - `user_responsible`: Usuario responsable de la carga
  - `publication`: Peri√≥dico (db, dm, sm, etc.)
  - `start_date`: Fecha inicio
  - `end_date`: Fecha final
- **Librer√≠a PortAda**:
  ```python
  df_dup = metadata.read_log("duplicates_log")
  # Aplicar filtros
  df_dup = df_dup.filter(...)
  ```
- **Retorna**: Vista maestro con metadatos de duplicados por d√≠a/edici√≥n

- **Endpoint Detalle**: `GET /api/analysis/duplicates/{log_id}/details`
- **Par√°metros**:
  - `duplicates_filter`: Filtro de la fila seleccionada
  - `duplicate_ids`: IDs de duplicados (comma-separated)
- **Librer√≠a PortAda**:
  ```python
  df_duplicates = metadata.read_log("duplicates_records")
  filtered = df_duplicates.filter(duplicates_filter)
                          .filter(df_duplicates.entry_id.isin(duplicate_ids))
  ```
- **Retorna**: Registros duplicados detallados

#### 2.3 Metadatos de Almacenamiento
- **Endpoint Master**: `POST /api/analysis/storage-metadata`
- **Par√°metros** (opcionales):
  - `table_name`: Nombre de la tabla (ej: "ship_entries")
  - `process`: Nombre del proceso responsable
- **Filtro Autom√°tico**: `stage = 0` (siempre)
- **Librer√≠a PortAda**:
  ```python
  df_storage = metadata.read_log("storage_log")
  df_storage = df_storage.filter("stage == 0")
  # Aplicar filtros adicionales
  ```
- **Retorna**: Vista maestro con metadatos de almacenamiento

- **Endpoint Detalle**: `GET /api/analysis/storage-metadata/{log_id}/lineage`
- **Librer√≠a PortAda**:
  ```python
  df_lineage = metadata.read_log("field_lineage_log")
  df_lineage = df_lineage.filter(df_lineage.stored_log_id == log_id)
  ```
- **Retorna**: Linaje de campos (cambios realizados en el dataframe)

#### 2.4 Metadatos de Procesos
- **Endpoint**: `POST /api/analysis/process-metadata`
- **Par√°metros**:
  - `process_name`: Nombre del proceso (opcional)
  - **Default**: Filtra por `process = 'ingest.save_raw_data'`
- **Filtro Autom√°tico**: `stage = 0` (siempre)
- **Librer√≠a PortAda**:
  ```python
  df_process = metadata.read_log("process_log")
  df_process = df_process.filter("process = 'ingest.save_raw_data'")
  ```
- **Retorna**: Metadatos de procesos ejecutados con errores si aplica

---

## üé® FRONTEND (React + TypeScript)

### 1. Vista de Ingesta (`/ingestion`)

#### Caracter√≠sticas Implementadas (seg√∫n rectificaci√≥n):
- ‚úÖ **Dropdown de selecci√≥n** de tipo de ingesta:
  - Extracci√≥n
  - Entidades Conocidas
- ‚úÖ **Upload √∫nico** seg√∫n tipo seleccionado (no simult√°neo)
- ‚úÖ **Validaci√≥n de archivos**: .json, .yml, .yaml
- ‚úÖ **Respuesta simplificada**: Mensaje de confirmaci√≥n/error
- ‚úÖ **Procesamiento as√≠ncrono**: Background tasks
- ‚úÖ **Estados de upload**:
  - idle: Sin archivo
  - uploading: Subiendo (con barra de progreso)
  - success: Completado
  - error: Error

#### Flujo de Usuario:
1. Selecciona tipo de ingesta (dropdown)
2. Arrastra/selecciona archivo
3. Ve progreso de upload
4. Recibe confirmaci√≥n
5. Procesamiento contin√∫a en background

---

### 2. Vista de An√°lisis

#### 2.1 Fechas Faltantes (`/analysis/missing-dates`)

**Caracter√≠sticas**:
- ‚úÖ **Selecci√≥n obligatoria** de peri√≥dico (DB, DM, SM)
- ‚úÖ **Dos modos de consulta**:
  
  **Modo 1: Archivo**
  - Upload de archivo con lista de fechas/ediciones
  - Formatos soportados:
    - **YAML**: 
      ```yaml
      1850-10-01:
        - U
      1850-10-02:
        - M
        - T
      ```
    - **JSON**: 
      ```json
      [{"1850-10-01":["U"]}, {"1850-10-02":["M","T"]}]
      ```
    - **Lista**: Una fecha por l√≠nea
  
  **Modo 2: Rango de Fechas**
  - Fecha inicio (opcional)
  - Fecha final (opcional)
  - Formato: YYYY-MM-DD

- ‚úÖ **Resultados con scroll**: Lista puede ser muy larga
- ‚úÖ **Informaci√≥n del gap**: Duraci√≥n de cada falta

---

#### 2.2 Duplicados (`/analysis/duplicates`)

**Caracter√≠sticas**:
- ‚úÖ **Filtros opcionales**:
  - Usuario responsable
  - Peri√≥dico (db, dm, sm)
  - Rango de fechas
- ‚úÖ **Vista Master-Detail**:
  - **Master**: Tabla con metadatos por d√≠a/edici√≥n
    - log_id
    - fecha
    - edici√≥n
    - publicaci√≥n
    - usuario
    - cantidad de duplicados
  - **Detail**: Expandible por fila
    - Registros duplicados espec√≠ficos
    - Contenido completo
    - Score de similitud

---

#### 2.3 Metadatos de Almacenamiento (`/analysis/storage-metadata`)

**Caracter√≠sticas**:
- ‚úÖ **Filtros opcionales**:
  - Nombre de tabla (ej: "ship_entries")
  - Proceso
- ‚úÖ **Filtro autom√°tico**: stage = 0
- ‚úÖ **Vista Master-Detail**:
  - **Master**: Metadatos de almacenamiento
    - log_id
    - nombre de tabla
    - proceso
    - timestamp
    - cantidad de registros
  - **Detail**: Field Lineage
    - Nombre del campo
    - Operaci√≥n realizada
    - Valor anterior
    - Valor nuevo
    - Timestamp

---

#### 2.4 Metadatos de Procesos (`/analysis/process-metadata`)

**Caracter√≠sticas**:
- ‚úÖ **Filtro opcional**: Nombre del proceso
- ‚úÖ **Filtro por defecto**: `process = 'ingest.save_raw_data'`
- ‚úÖ **Filtro autom√°tico**: stage = 0
- ‚úÖ **Informaci√≥n mostrada**:
  - log_id
  - proceso
  - timestamp
  - duraci√≥n
  - estado (success/error)
  - registros procesados
  - mensaje de error (si aplica)

---

## üîß Integraci√≥n PortAda

### Clases Utilizadas:
```python
from portada_data_layer import PortadaBuilder, DataLakeMetadataManager

# Builder
builder = (
    PortadaBuilder()
    .protocol("file://")
    .base_path(base_path)
    .app_name(app_name)
    .project_name(project_name)
)

# Capas de datos
layer_news = builder.build(builder.NEWS_TYPE)
layer_entities = builder.build(builder.KNOWN_ENTITIES_TYPE)

# Metadata Manager
metadata = DataLakeMetadataManager(layer_news.get_configuration())
```

### Logs Disponibles:
- `duplicates_log`: Metadatos de duplicados
- `duplicates_records`: Registros duplicados detallados
- `storage_log`: Metadatos de almacenamiento
- `field_lineage_log`: Linaje de campos
- `process_log`: Metadatos de procesos

---

## ‚úÖ Cumplimiento de Rectificaciones

### Ingesta:
- ‚úÖ Separaci√≥n de procesos (dropdown en lugar de uploads simult√°neos)
- ‚úÖ Respuesta simplificada (as√≠ncrona)
- ‚úÖ Almacenamiento temporal en estructura de carpetas

### An√°lisis:
- ‚úÖ Dropdown en sidebar con 4 tipos de an√°lisis
- ‚úÖ Pantallas individuales para cada consulta
- ‚úÖ Vistas master-detail donde corresponde
- ‚úÖ Filtros opcionales implementados
- ‚úÖ Filtros autom√°ticos (stage = 0) aplicados

---

## üöÄ Estado de Implementaci√≥n

**Backend**: ‚úÖ 100% Funcional
- Todos los endpoints implementados
- Integraci√≥n con PortAda v0.1.3
- Procesamiento as√≠ncrono
- Validaciones y manejo de errores

**Frontend**: ‚úÖ 100% Implementado
- Todas las vistas seg√∫n especificaci√≥n
- UI/UX moderna y responsive
- Manejo de estados y errores
- Integraci√≥n con API backend

**Documentaci√≥n**: ‚úÖ Completa
- API docs auto-generada (Swagger/ReDoc)
- Gu√≠as de integraci√≥n
- Verificaci√≥n de endpoints

---

## üìù Notas Importantes

1. **Archivos de ingesta**: Se eliminan despu√©s del procesamiento (comportamiento de PortAda)
2. **Procesamiento as√≠ncrono**: Usar endpoint de status para monitorear
3. **Python 3.12+**: Requerido por la librer√≠a PortAda
4. **Java**: Necesario para PySpark (dependencia de PortAda)
5. **Delta Lake**: Formato de almacenamiento subyacente
